{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596920419783",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Ensemble Neural Network Approach\n",
    "\n",
    "### This notebook shows how to train Ensemble Neural Network that is built upon Metric learning and CNN structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_value = -1\n",
    "sequencelength = 45\n",
    "\n",
    "bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "   'B8A', 'B9', 'QA10', 'QA20', 'QA60', 'doa']\n",
    "\n",
    "selected_bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9']\n",
    "\n",
    "selected_band_idxs = np.array([bands.index(b) for b in selected_bands])\n",
    "\n",
    "def transform(x):\n",
    "    x = x[x[:, 0] != padded_value, :] # remove padded values\n",
    "    \n",
    "    # choose selected bands\n",
    "    x = x[:,selected_band_idxs] * 1e-4 # scale reflectances to 0-1\n",
    "\n",
    "    # choose with replacement if sequencelength smaller als choose_t\n",
    "    replace = False if x.shape[0] >= sequencelength else True\n",
    "    idxs = np.random.choice(x.shape[0], sequencelength, replace=replace)\n",
    "    idxs.sort()\n",
    "\n",
    "    x = x[idxs]\n",
    "\n",
    "    return torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "def target_transform(y):\n",
    "    y = frh01.mapping.loc[y].id\n",
    "    return torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "data_path = \"path_to_breizhcrops_dataset\"\n",
    "data_path = \"/home/firatk/Desktop/finalproject/breizhcrops_data\"\n",
    "\n",
    "# load training data\n",
    "frh01 = dataset.BreizhDataset(region=\"frh01\", root=data_path, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)\n",
    "frh02 = dataset.BreizhDataset(region=\"frh02\", root=data_path, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)\n",
    "\n",
    "frh01.isClassification = True\n",
    "frh02.isClassification = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LSTM, ClassificationModel, CnnNet, EnsembleNet\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "hidden_dims = 128\n",
    "num_layers = 3\n",
    "num_classes = 13\n",
    "input_dim = 13\n",
    "bidirectional = True\n",
    "\n",
    "lstm = LSTM(input_dim=input_dim, hidden_dims=hidden_dims, num_classes=num_classes, num_layers=num_layers, dropout=0.2, bidirectional=True, use_layernorm=True)\n",
    "lstm.load(\"lstm_model_path.pth\")\n",
    "\n",
    "classifier = ClassificationModel((hidden_dims + hidden_dims * bidirectional) * num_layers, hidden_dims ,num_classes)\n",
    "classifier.load(\"ann_model_path.pth\")\n",
    "\n",
    "cnn = CnnNet()\n",
    "cnn.load(\"cnn_model_path.pth\")\n",
    "\n",
    "ensemble = EnsembleNet(lstm, classifier, cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below shows the traning part of the Ensemble Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data = torch.utils.data.ConcatDataset([frh01,frh02])\n",
    "trainingDataLoader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle=True,num_workers=0)\n",
    "\n",
    "# training configuration\n",
    "epochs = 101\n",
    "lrDecreaseStep = 5\n",
    "earlyStoppingStep = 2\n",
    "\n",
    "loss = 0\n",
    "minLoss = 999\n",
    "lossNotDecreasedCounter = 0\n",
    "lrDecreasedCounter = 0\n",
    "\n",
    "ensemble.train()\n",
    "if torch.cuda.is_available():\n",
    "    ensemble.cuda()\n",
    "\n",
    "# define loss function and the optimizer\n",
    "lossFn = CrossEntropyLoss()\n",
    "optimizer = optim.SGD(ensemble.parameters(), lr=0.001, momentum=0.5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epochLoss = 0\n",
    "    for (batch_id,data) in enumerate(trainingDataLoader):\n",
    "\n",
    "        # consider only one element, instead of pair like in siamese structure\n",
    "        x1, _, labels = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          x1 = x1.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "        # forward once over the input element\n",
    "        out1 = ensemble.forward(x1)\n",
    "\n",
    "        loss = lossFn(out1, labels)\n",
    "        epochLoss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"Iteration {}: loss {:.2f}\".format(batch_id,loss.item()))\n",
    "\n",
    "    batchId = (batch_id + 1)\n",
    "    avgLoss = epochLoss / batchId\n",
    "\n",
    "    # check the loss to decide early stopping or learning rate decreasing\n",
    "    if avgLoss < minLoss:\n",
    "        lossNotDecreasedCounter = 0\n",
    "        minLoss = avgLoss\n",
    "    else:\n",
    "        lossNotDecreasedCounter +=1\n",
    "\n",
    "    if lossNotDecreasedCounter == lrDecreaseStep:\n",
    "        lossNotDecreasedCounter = 0\n",
    "        minLoss = 999\n",
    "        lrDecreasedCounter +=1\n",
    "        print(\"Decrease learning rate...\")\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr']\n",
    "            g['lr'] = lr * 0.1\n",
    "\n",
    "    if lrDecreasedCounter == earlyStoppingStep:\n",
    "        print(\"Earyl stopping...\")\n",
    "        print(\"Saving the model...\")\n",
    "        torch.save(cnn.state_dict(),\"cnn_model.pth\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(\"Saving....\")\n",
    "      torch.save(cnn.state_dict(),\"cnn_model.pth\")\n",
    "\n",
    "    print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,avgLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}