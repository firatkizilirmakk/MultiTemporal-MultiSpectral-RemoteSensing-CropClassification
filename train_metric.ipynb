{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Metric Learning Approach\n",
    "\n",
    "### This notebook shows how to train metric learning approach with the Siamese LSTM network and the Artificial Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_value = -1\n",
    "sequencelength = 45\n",
    "\n",
    "bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8',\n",
    "   'B8A', 'B9', 'QA10', 'QA20', 'QA60', 'doa']\n",
    "\n",
    "selected_bands = ['B1', 'B10', 'B11', 'B12', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9']\n",
    "\n",
    "selected_band_idxs = np.array([bands.index(b) for b in selected_bands])\n",
    "\n",
    "def transform(x):\n",
    "    x = x[x[:, 0] != padded_value, :] # remove padded values\n",
    "    \n",
    "    # choose selected bands\n",
    "    x = x[:,selected_band_idxs] * 1e-4 # scale reflectances to 0-1\n",
    "\n",
    "    # choose with replacement if sequencelength smaller als choose_t\n",
    "    replace = False if x.shape[0] >= sequencelength else True\n",
    "    idxs = np.random.choice(x.shape[0], sequencelength, replace=replace)\n",
    "    idxs.sort()\n",
    "\n",
    "    x = x[idxs]\n",
    "\n",
    "    return torch.from_numpy(x).type(torch.FloatTensor).to(device)\n",
    "\n",
    "def target_transform(y):\n",
    "    y = frh01.mapping.loc[y].id\n",
    "    return torch.tensor(y, dtype=torch.long, device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "data_path = \"path_to_breizhcrops_dataset\"\n",
    "\n",
    "# load training data\n",
    "frh01 = dataset.BreizhDataset(region=\"frh01\", root=data_path, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)\n",
    "frh02 = dataset.BreizhDataset(region=\"frh02\", root=data_path, transform=transform,\n",
    "                                target_transform=target_transform, padding_value=padded_value)   \n",
    "\n",
    "# select elements of the same classes from different regions\n",
    "frh01.setOtherDataset(frh02)\n",
    "frh02.setOtherDataset(frh01)                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import LSTM, ClassificationModel\n",
    "from loss import ContrastiveLoss\n",
    "\n",
    "# model configurations\n",
    "hidden_dims = 128\n",
    "num_layers = 3\n",
    "num_classes = 13\n",
    "input_dim = 13\n",
    "bidirectional = True\n",
    "\n",
    "# create Siamese LSTM model with these configurations\n",
    "lstm = LSTM(input_dim=input_dim, hidden_dims=hidden_dims, num_classes=num_classes, num_layers=num_layers, dropout=0.2, bidirectional=True, use_layernorm=True)\n",
    "classifier = ClassificationModel((hidden_dims + hidden_dims * bidirectional) * num_layers, hidden_dims ,num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below shows the traning part of the Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = torch.utils.data.ConcatDataset([frh01,frh02])\n",
    "trainingDataLoader = torch.utils.data.DataLoader(data,batch_size=64,shuffle=True,num_workers=0)\n",
    "\n",
    "# training configuration\n",
    "epochs = 101\n",
    "lrDecreaseStep = 5\n",
    "earlyStoppingStep = 2\n",
    "\n",
    "loss = 0\n",
    "minLoss = 999\n",
    "lossNotDecreasedCounter = 0\n",
    "lrDecreasedCounter = 0\n",
    "\n",
    "\n",
    "lstm.train()\n",
    "if torch.cuda.is_available():\n",
    "    lstm.cuda()\n",
    "\n",
    "# create loss function and optimizer\n",
    "lossFn = ContrastiveLoss(margin = 3)\n",
    "optimizer = torch.optim.Adam(\n",
    "        filter(lambda x: x.requires_grad, lstm.parameters()),\n",
    "        betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-6)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epochLoss = 0\n",
    "    for (batch_id,data) in enumerate(trainingDataLoader):\n",
    "\n",
    "        # get paired data with corresponding label (1 for positive, 0 for negative)\n",
    "        x1, x2, labels = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          x1 = x1.cuda()\n",
    "          x2 = x2.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "        # forward once over the pair\n",
    "        out1, out2 = lstm.forward(x1, x2)\n",
    "\n",
    "        loss = lossFn(out1, out2, labels)\n",
    "        epochLoss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"Iteration {}: loss {:.2f}\".format(batch_id,loss.item()))\n",
    "\n",
    "    batchId = (batch_id + 1)\n",
    "    avgLoss = epochLoss / batchId\n",
    "\n",
    "    # check the loss to decide early stopping or learning rate decreasing\n",
    "    if avgLoss < minLoss - 0.01:\n",
    "        lossNotDecreasedCounter = 0\n",
    "        minLoss = avgLoss\n",
    "    else:\n",
    "        lossNotDecreasedCounter +=1\n",
    "\n",
    "    if lossNotDecreasedCounter > lrDecreaseStep:\n",
    "        lossNotDecreasedCounter = 0\n",
    "        minLoss = 999\n",
    "        lrDecreasedCounter +=1\n",
    "        print(\"Decrease learning rate...\")\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr']\n",
    "            g['lr'] = lr * 0.1\n",
    "\n",
    "    if lrDecreasedCounter == earlyStoppingStep:\n",
    "        print(\"Earyl stopping...\")\n",
    "        torch.save(lstm.state_dict(),\"lstm_model.pth\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(\"Saving....\")\n",
    "      torch.save(lstm.state_dict(),\"lstm_model.pth\")\n",
    "\n",
    "    print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,avgLoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below shows the traning part of the Artificial Neural Network fed by Siamese LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the trained lstm model\n",
    "lstm.load(\"path_to_lstm_model.pth\")\n",
    "\n",
    "# set dataset to return proper label of the classes instead of 1 and 0\n",
    "frh01.isClassification = True\n",
    "frh02.isClassification = True\n",
    "\n",
    "data = torch.utils.data.ConcatDataset([frh01,frh02])\n",
    "trainingDataLoader = torch.utils.data.DataLoader(data,batch_size=64,shuffle=True,num_workers=0)\n",
    "\n",
    "# training configuration\n",
    "epochs = 101\n",
    "lrDecreaseStep = 5\n",
    "earlyStoppingStep = 2\n",
    "\n",
    "loss = 0\n",
    "minLoss = 999\n",
    "lossNotDecreasedCounter = 0\n",
    "lrDecreasedCounter = 0\n",
    "\n",
    "classifier.train()\n",
    "if torch.cuda.is_available():\n",
    "    lstm = lstm.cuda()\n",
    "    classifier = classifier.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "        filter(lambda x: x.requires_grad, classifier.parameters()),\n",
    "        betas=(0.9, 0.98), eps=1e-09)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epochLoss = 0\n",
    "    for (batch_id,data) in enumerate(trainingDataLoader):\n",
    "\n",
    "        # get paired data with corresponding label of x1 instead of 1 and 0\n",
    "        x1, x2, labels = data\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          x1 = x1.cuda()\n",
    "          x2 = x2.cuda()\n",
    "          labels = labels.cuda()\n",
    "\n",
    "        # forward once over LSTM and forward first output over ANN\n",
    "        out1, out2 = lstm.forward(x1,x2)\n",
    "        logProbs = classifier.forward(out1)\n",
    "\n",
    "        loss = torch.nn.functional.nll_loss(logProbs, labels)\n",
    "        epochLoss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"Iteration {}: loss {:.2f}\".format(batch_id,loss.item()))\n",
    "\n",
    "    batchId = (batch_id + 1)\n",
    "    avgLoss = epochLoss / batchId\n",
    "\n",
    "    # check the loss to decide early stopping or learning rate decreasing\n",
    "    if avgLoss < minLoss - 0.01:\n",
    "        lossNotDecreasedCounter = 0\n",
    "        minLoss = avgLoss\n",
    "    else:\n",
    "        lossNotDecreasedCounter +=1\n",
    "\n",
    "    if lossNotDecreasedCounter > lrDecreaseStep:\n",
    "        lossNotDecreasedCounter = 0\n",
    "        minLoss = 999\n",
    "        lrDecreasedCounter +=1\n",
    "        print(\"Decrease learning rate...\")\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            lr = g['lr']\n",
    "            g['lr'] = lr * 0.1\n",
    "\n",
    "    if lrDecreasedCounter == earlyStoppingStep:\n",
    "        print(\"Earyl stopping...\")\n",
    "        print(\"Saving the model...\")\n",
    "        torch.save(classifier.state_dict(),\"ann_model.pth\")\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "      print(\"Saving....\")\n",
    "      torch.save(classifier.state_dict(),\"ann_model.pth\")\n",
    "\n",
    "    print(\"Epoch number {}\\n Current loss {}\\n\".format(epoch,avgLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitenvconda225765b0c72f4bd69b9c7ce70b04c535",
   "display_name": "Python 3.8.1 64-bit ('env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}